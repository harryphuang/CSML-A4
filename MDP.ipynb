{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65db0e5e",
   "metadata": {},
   "source": [
    "Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81143b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map, FrozenLakeEnv\n",
    "import glob\n",
    "from hiive.mdptoolbox.mdp import ValueIteration, PolicyIteration, QLearning\n",
    "from hiive.mdptoolbox.example import forest\n",
    "# import hiive_mdptoolbox.example\n",
    "# import hiive_mdptoolbox\n",
    "import numpy as np\n",
    "import os\n",
    "from numpy.random import choice\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "np.random.seed(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089650e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "twenty = generate_random_map(20)\n",
    "MAPS = {\n",
    "    \"20x20\": twenty\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624708c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_process(env, policy, gamma, render = True):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "def evaluate_policy(env, policy, gamma , n = 100):\n",
    "    scores = [run_process(env, policy, gamma, False) for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def get_policy(env,v, gamma):\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            q_sa[a] = sum([p * (r + gamma * v[s_]) for p, s_, r, _ in  env.P[s][a]])\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "def compute_policy(env, policy, gamma):\n",
    "    v = np.zeros(env.nS)\n",
    "    eps = 1e-5\n",
    "    while True:\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            policy_a = policy[s]\n",
    "            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, is_done in env.P[s][policy_a]])\n",
    "        if (np.sum((np.fabs(prev_v - v))) <= eps):\n",
    "            break\n",
    "    return v\n",
    "\n",
    "def run_policy_iteration(env, gamma):\n",
    "    policy = np.random.choice(env.nA, size=(env.nS))  \n",
    "    max_iters = 200000\n",
    "    desc = env.unwrapped.desc\n",
    "    for i in range(max_iters):\n",
    "        old_policy_v = compute_policy(env, policy, gamma)\n",
    "        new_policy = get_policy(env,old_policy_v, gamma)\n",
    "        if (np.all(policy == new_policy)):\n",
    "            k=i+1\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy,k\n",
    "\n",
    "def run_value_iteration(env, gamma):\n",
    "    v = np.zeros(env.nS)  # initialize value-function\n",
    "    max_iters = 100000\n",
    "    eps = 1e-20\n",
    "    desc = env.unwrapped.desc\n",
    "    for i in range(max_iters):\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            q_sa = [sum([p*(r + gamma*prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)] \n",
    "            v[s] = max(q_sa)\n",
    "        if (np.sum(np.fabs(prev_v - v)) <= eps):\n",
    "            k=i+1\n",
    "            break\n",
    "    return v,k\n",
    "\n",
    "def show_policy_map(title, policy, map_desc, color_map, direction_map):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, xlim=(0, policy.shape[1]), ylim=(0, policy.shape[0]))\n",
    "    font_size = 'x-large'\n",
    "    if policy.shape[1] > 16:\n",
    "        font_size = 'small'\n",
    "    plt.title(title)\n",
    "    for i in range(policy.shape[0]):\n",
    "        for j in range(policy.shape[1]):\n",
    "            y = policy.shape[0] - i - 1\n",
    "            x = j\n",
    "            p = plt.Rectangle([x, y], 1, 1)\n",
    "            p.set_facecolor(color_map[map_desc[i,j]])\n",
    "            ax.add_patch(p)\n",
    "\n",
    "            text = ax.text(x+0.5, y+0.5, direction_map[policy[i, j]], weight='bold', size=font_size,\n",
    "                           horizontalalignment='center', verticalalignment='center', color='w')\n",
    "            \n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.xlim((0, policy.shape[1]))\n",
    "    plt.ylim((0, policy.shape[0]))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(title+str('.png'))\n",
    "    plt.close()\n",
    "\n",
    "    return (plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c1f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_Frozen_Lake(option):\n",
    "    # 0 = left; 1 = down; 2 = right;  3 = up\n",
    "    size = 4\n",
    "    if (option == \"4x4\"):\n",
    "        environment  = 'FrozenLake-v1'\n",
    "        env = gym.make(environment)\n",
    "        size = 4\n",
    "    else:\n",
    "        env = FrozenLakeEnv(desc=MAPS[\"20x20\"])\n",
    "        size = 20\n",
    "    env = env.unwrapped\n",
    "    desc = env.unwrapped.desc\n",
    "    time_array=[0]*10\n",
    "    gamma_arr=[0]*10\n",
    "    iters=[0]*10\n",
    "    list_scores=[0]*10\n",
    "\n",
    "    \n",
    "    ### POLICY ITERATION ####\n",
    "    print('POLICY ITERATION WITH FROZEN LAKE ' + option)\n",
    "    for i in range(0,10):\n",
    "        st=time.time()\n",
    "        best_policy,k = run_policy_iteration(env, gamma = (i+0.5)/10)\n",
    "        scores = evaluate_policy(env, best_policy, gamma = (i+0.5)/10)\n",
    "        plot = show_policy_map('Frozen Lake  ' + option + ' Policy Map Iteration '+ str(i) + ' (Policy Iteration) ' + 'i: '+ str(i),best_policy.reshape(size,size),desc,colors(),directions())\n",
    "        end=time.time()\n",
    "        gamma_arr[i]=(i+0.5)/10\n",
    "        list_scores[i]=np.mean(scores)\n",
    "        iters[i] = k\n",
    "        time_array[i]=end-st\n",
    "    \n",
    "    # print('Frozen Lake ' + option + ' - Policy Iteration')\n",
    "    # print(list_scores)\n",
    "    \n",
    "    plt.plot(gamma_arr, time_array)\n",
    "    plt.xlabel('Gammas')\n",
    "    plt.title('Frozen Lake ' + option + '- Policy Iteration - Execution Time Analysis')\n",
    "    plt.ylabel('Execution Time (s)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(gamma_arr,list_scores)\n",
    "    plt.xlabel('Gammas')\n",
    "    plt.ylabel('Average Rewards')\n",
    "    plt.title('Frozen Lake ' + option + ' - Policy Iteration - Reward Analysis')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(gamma_arr,iters)\n",
    "    plt.xlabel('Gammas')\n",
    "    plt.ylabel('Iterations to Converge')\n",
    "    plt.title('Frozen Lake ' + option + ' - Policy Iteration - Convergence Analysis')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    ### VALUE ITERATION ###\n",
    "    print('VALUE ITERATION WITH FROZEN LAKE ' + option)\n",
    "    best_vals=[0]*10\n",
    "    for i in range(0,10):\n",
    "        st=time.time()\n",
    "        best_value,k = run_value_iteration(env, gamma = (i+0.5)/10)\n",
    "        policy = get_policy(env,best_value, gamma = (i+0.5)/10)\n",
    "        policy_score = evaluate_policy(env, policy, gamma=(i+0.5)/10, n=1000)\n",
    "        gamma = (i+0.5)/10\n",
    "        plot = show_policy_map('Frozen Lake  ' + option + ' Policy Map Iteration '+ str(i) + ' (Value Iteration) ' + 'Gamma: '+ str(gamma),policy.reshape(size,size),desc,colors(),directions())\n",
    "        end=time.time()\n",
    "        gamma_arr[i]=(i+0.5)/10\n",
    "        iters[i]=k\n",
    "        best_vals[i] = best_value\n",
    "        list_scores[i]=np.mean(policy_score)\n",
    "        time_array[i]=end-st\n",
    "\n",
    "        \n",
    "    # print('Frozen Lake ' + option + ' - Value Iteration')\n",
    "    # print(list_scores)\n",
    "    \n",
    "    plt.plot(gamma_arr, time_array)\n",
    "    plt.xlabel('Gammas')\n",
    "    plt.title('Frozen Lake ' + option + ' - Value Iteration - Execution Time Analysis')\n",
    "    plt.ylabel('Execution Time (s)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(gamma_arr,list_scores)\n",
    "    plt.xlabel('Gammas')\n",
    "    plt.ylabel('Average Rewards')\n",
    "    plt.title('Frozen Lake ' + option + ' - Value Iteration - Reward Analysis')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(gamma_arr,iters)\n",
    "    plt.xlabel('Gammas')\n",
    "    plt.ylabel('Iterations to Converge')\n",
    "    plt.title('Frozen Lake ' + option + ' - Value Iteration - Convergence Analysis')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(gamma_arr,best_vals)\n",
    "    plt.xlabel('Gammas')\n",
    "    plt.ylabel('Optimal Value')\n",
    "    plt.legend(['epsilon=0.05','epsilon=0.15','epsilon=0.25','epsilon=0.50','epsilon=0.75','epsilon=0.95'])\n",
    "    plt.title('Frozen Lake ' + option + ' - Value Iteration - Best Value Analysis')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    ### Q-LEARNING #####\n",
    "    print('Q LEARNING WITH FROZEN LAKE ' + option)\n",
    "    st = time.time()\n",
    "    reward_array = []\n",
    "    iter_array = []\n",
    "    size_array = []\n",
    "    chunks_array = []\n",
    "    averages_array = []\n",
    "    time_array = []\n",
    "    Q_array = []\n",
    "    for epsilon in [0.05,0.15,0.25,0.5,0.75,0.90]:\n",
    "        Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        rewards = []\n",
    "        iters = []\n",
    "        optimal=[0]*env.observation_space.n\n",
    "        alpha = 0.85\n",
    "        gamma = 0.95\n",
    "        episodes = 30000\n",
    "        \n",
    "        if (option == \"4x4\"):\n",
    "            environment  = 'FrozenLake-v1'\n",
    "            env = gym.make(environment)\n",
    "        else:\n",
    "            env = FrozenLakeEnv(desc=MAPS[\"20x20\"])\n",
    "\n",
    "        env = env.unwrapped\n",
    "        desc = env.unwrapped.desc\n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            t_reward = 0\n",
    "            max_steps = 1000000\n",
    "            for i in range(max_steps):\n",
    "                if done:\n",
    "                    break        \n",
    "                current = state\n",
    "                if np.random.rand() < (epsilon):\n",
    "                    action = np.argmax(Q[current, :])\n",
    "                else:\n",
    "                    action = env.action_space.sample()\n",
    "                \n",
    "                state, reward, done, info = env.step(action)\n",
    "                t_reward += reward\n",
    "                Q[current, action] += alpha * (reward + gamma * np.max(Q[state, :]) - Q[current, action])\n",
    "            epsilon=(1-2.71**(-episode/1000))\n",
    "            rewards.append(t_reward)\n",
    "            iters.append(i)\n",
    "\n",
    "\n",
    "        for k in range(env.observation_space.n):\n",
    "            optimal[k]=np.argmax(Q[k, :])\n",
    "\n",
    "        reward_array.append(rewards)\n",
    "        iter_array.append(iters)\n",
    "        Q_array.append(Q)\n",
    "\n",
    "        env.close()\n",
    "        end=time.time()\n",
    "        time_array.append(end-st)\n",
    "\n",
    "        # Plot results\n",
    "        def chunk_list(l, n):\n",
    "            for i in range(0, len(l), n):\n",
    "                yield l[i:i + n]\n",
    "\n",
    "        size = int(episodes / 50)\n",
    "        chunks = list(chunk_list(rewards, size))\n",
    "        averages = [sum(chunk) / len(chunk) for chunk in chunks]\n",
    "        size_array.append(size)\n",
    "        chunks_array.append(chunks)\n",
    "        averages_array.append(averages)\n",
    "        \n",
    "    # print('Frozen Lake ' + option + ' - Q Learning Q Array')\n",
    "    # print(Q_array)\n",
    "\n",
    "    # print('Frozen Lake ' + option + ' - Q Learning Reward Array')\n",
    "    # print(reward_array)\n",
    "\n",
    "    plt.plot(range(0, len(reward_array[0]), size_array[0]), averages_array[0],label='epsilon=0.05')\n",
    "    plt.plot(range(0, len(reward_array[1]), size_array[1]), averages_array[1],label='epsilon=0.15')\n",
    "    plt.plot(range(0, len(reward_array[2]), size_array[2]), averages_array[2],label='epsilon=0.25')\n",
    "    plt.plot(range(0, len(reward_array[3]), size_array[3]), averages_array[3],label='epsilon=0.50')\n",
    "    plt.plot(range(0, len(reward_array[4]), size_array[4]), averages_array[4],label='epsilon=0.75')\n",
    "    plt.plot(range(0, len(reward_array[5]), size_array[5]), averages_array[5],label='epsilon=0.95')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.grid()\n",
    "    plt.title('Frozen Lake ' + option + ' - Q Learning - Constant Epsilon')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot([0.05,0.15,0.25,0.5,0.75,0.95],time_array)\n",
    "    plt.xlabel('Epsilon Values')\n",
    "    plt.grid()\n",
    "    plt.title('Frozen Lake ' + option + ' - Q Learning')\n",
    "    plt.ylabel('Execution Time (s)')\n",
    "    plt.show()\n",
    "\n",
    "    plt.subplot(1,6,1)\n",
    "    plt.imshow(Q_array[0])\n",
    "    plt.title('Epsilon=0.05')\n",
    "\n",
    "    plt.subplot(1,6,2)\n",
    "    plt.title('Epsilon=0.15')\n",
    "    plt.imshow(Q_array[1])\n",
    "\n",
    "    plt.subplot(1,6,3)\n",
    "    plt.title('Epsilon=0.25')\n",
    "    plt.imshow(Q_array[2])\n",
    "\n",
    "    plt.subplot(1,6,4)\n",
    "    plt.title('Epsilon=0.50')\n",
    "    plt.imshow(Q_array[3])\n",
    "\n",
    "    plt.subplot(1,6,5)\n",
    "    plt.title('Epsilon=0.75')\n",
    "    plt.imshow(Q_array[4])\n",
    "\n",
    "    plt.subplot(1,6,6)\n",
    "    plt.title('Epsilon=0.95')\n",
    "    plt.imshow(Q_array[5])\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f52fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colors():\n",
    "    return {\n",
    "        b'S': 'green',\n",
    "        b'F': 'skyblue',\n",
    "        b'H': 'black',\n",
    "        b'G': 'gold',\n",
    "    }\n",
    "\n",
    "def directions():\n",
    "    return {\n",
    "        3: '⬆',\n",
    "        2: '➡',\n",
    "        1: '⬇',\n",
    "        0: '⬅'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fb6c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('STARTING FROZEN LAKE 4X4')\n",
    "run_Frozen_Lake(\"4x4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8cf9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('STARTING FROZEN LAKE 20X20')\n",
    "run_Frozen_Lake(\"20x20\")\n",
    "print('END OF RUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79d775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in glob.glob(\"/kaggle/working/*.png\"):\n",
    "    img = mpimg.imread(image_path)\n",
    "    plt.ion()\n",
    "    plt.figure()\n",
    "    plt.axis('off') \n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f7490b",
   "metadata": {},
   "source": [
    "Forest Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f41ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R = forest(S=400, r1=5, r2= 2, p=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32899dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e759f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(P, R, policy, test_count=100, gamma=0.9):\n",
    "    num_state = P.shape[-1]\n",
    "    total_episode = num_state * test_count\n",
    "    # start in each state\n",
    "    total_reward = 0\n",
    "    for state in range(num_state):\n",
    "        state_reward = 0\n",
    "        for state_episode in range(test_count):\n",
    "            episode_reward = 0\n",
    "            disc_rate = 1\n",
    "            while True:\n",
    "                # take step\n",
    "                action = policy[state]\n",
    "                # get next step using P\n",
    "                probs = P[action][state]\n",
    "                candidates = list(range(len(P[action][state])))\n",
    "                next_state =  choice(candidates, 1, p=probs)[0]\n",
    "                # get the reward\n",
    "                reward = R[state][action] * disc_rate\n",
    "                episode_reward += reward\n",
    "                # when go back to 0 ended\n",
    "                disc_rate *= gamma\n",
    "                if next_state == 0:\n",
    "                    break\n",
    "            state_reward += episode_reward\n",
    "        total_reward += state_reward\n",
    "    return total_reward / total_episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a175e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainVI(P, R, discount=0.9, epsilon=[1e-9]):\n",
    "    vi_df = pd.DataFrame(columns=[\"Epsilon\", \"Policy\", \"Iteration\", \n",
    "                                  \"Time\", \"Reward\", \"Value Function\"])\n",
    "    for eps in epsilon:\n",
    "        vi = ValueIteration(P, R, gamma=discount, epsilon=eps, max_iter=int(1e15))\n",
    "        vi.run()\n",
    "        reward = test_policy(P, R, vi.policy)\n",
    "        info = [float(eps), vi.policy, vi.iter, vi.time, reward, vi.V]\n",
    "        df_length = len(vi_df)\n",
    "        vi_df.loc[df_length] = info\n",
    "    return vi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f9f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_df = trainVI(P, R, epsilon=[1e-1, 1e-3, 1e-6, 1e-9, 1e-12, 1e-15])\n",
    "vi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82c0327",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = PolicyIteration(P, R, gamma=0.9, max_iter=1e6)\n",
    "pi.run()\n",
    "pi_pol = pi.policy\n",
    "pi_reward = test_policy(P, R, pi_pol)\n",
    "pi_iter = pi.iter\n",
    "pi_time = pi.time\n",
    "pi_iter, pi_time, pi_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bb3f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pi_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96221f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainQ(P, R, discount=0.9, alpha_dec=[0.99], alpha_min=[0.001], \n",
    "            epsilon=[1.0], epsilon_decay=[0.99], n_iter=[1000000]):\n",
    "    q_df = pd.DataFrame(columns=[\"Iterations\", \"Alpha Decay\", \"Alpha Min\", \n",
    "                                 \"Epsilon\", \"Epsilon Decay\", \"Reward\",\n",
    "                                 \"Time\", \"Policy\", \"Value Function\",\n",
    "                                 \"Training Rewards\"])\n",
    "    \n",
    "    count = 0\n",
    "    for i in n_iter:\n",
    "        for eps in epsilon:\n",
    "            for eps_dec in epsilon_decay:\n",
    "                for a_dec in alpha_dec:\n",
    "                    for a_min in alpha_min:\n",
    "                        q = QLearning(P, R, discount, alpha_decay=a_dec, \n",
    "                                      alpha_min=a_min, epsilon=eps, \n",
    "                                      epsilon_decay=eps_dec, n_iter=i)\n",
    "                        q.run()\n",
    "                        reward = test_policy(P, R, q.policy)\n",
    "                        count += 1\n",
    "                        print(\"{}: {}\".format(count, reward))\n",
    "                        st = q.run_stats\n",
    "                        rews = [s['Reward'] for s in st]\n",
    "                        info = [i, a_dec, a_min, eps, eps_dec, reward, \n",
    "                                q.time, q.policy, q.V, rews]\n",
    "                        \n",
    "                        df_length = len(q_df)\n",
    "                        q_df.loc[df_length] = info\n",
    "    return q_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2db381",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_decs = [0.99, 0.999]\n",
    "alpha_mins =[0.001, 0.0001]\n",
    "eps = [10.0, 1.0]\n",
    "eps_dec = [0.99, 0.999]\n",
    "iters = [1000000, 10000000]\n",
    "q_df = trainQ(P, R, discount=0.9, alpha_dec=alpha_decs, alpha_min=alpha_mins, \n",
    "            epsilon=eps, epsilon_decay=eps_dec, n_iter=iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6ea0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_df.Policy == pi_pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0289edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy(P,R,q_df.Policy[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c138772",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67d5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_df.groupby(\"Iterations\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a731299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_df.groupby(\"Epsilon Decay\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b215ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
